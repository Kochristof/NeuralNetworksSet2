////////////////////////////////////////////////////////////EXERCISE 6

function  BPANN()
    %---Set training parameters
    iterations = 5000;
    errorThreshhold = 0.1;
    learningRate = 0.5;
    %---Set hidden layer type, for example: [4, 3, 2]
    hiddenNeurons = [3 2];
    %---'Xor' training data
    trainInp = [0 0; 0 1; 1 0; 1 1];
    trainOut = [0; 1; 1; 0];
    testInp = trainInp;
    testRealOut = trainOut;
    % %---'And' training data
    % trainInp = [1 1; 1 0; 0 1; 0 0];
    % trainOut = [1; 0; 0; 0];
    % testInp = trainInp;
    % testRealOut = trainOut;
    assert(size(trainInp,1)==size(trainOut, 1),...
        'Counted different sets of input and output.');
    %---Initialize Network attributes
    inArgc = size(trainInp, 2);
    outArgc = size(trainOut, 2);
    trainsetCount = size(trainInp, 1);
    %---Add output layer
    layerOfNeurons = [hiddenNeurons, outArgc];
    layerCount = size(layerOfNeurons, 2);
    %---Weight and bias random range
    e = 1;
    b = -e;
    %---Set initial random weights
    weightCell = cell(1, layerCount);
    for i = 1:layerCount
        if i == 1
            weightCell{1} = unifrnd(b, e, inArgc,layerOfNeurons(1));
        else
            weightCell{i} = unifrnd(b, e, layerOfNeurons(i-1),layerOfNeurons(i));
        end
    end
    %---Set initial biases
    biasCell = cell(1, layerCount);
    for i = 1:layerCount
        biasCell{i} = unifrnd(b, e, 1, layerOfNeurons(i));
    end
    %----------------------
    %---Begin training
    %----------------------
    for iter = 1:iterations
        for i = 1:trainsetCount
            % choice = randi([1 trainsetCount]);
            choice = i;
            sampleIn = trainInp(choice, :);
            sampleTarget = trainOut(choice, :);
            [realOutput, layerOutputCells] = ForwardNetwork(sampleIn, layerOfNeurons, weightCell, biasCell);
            [weightCell, biasCell] = BackPropagate(learningRate, sampleIn, realOutput, sampleTarget, layerOfNeurons, ...
                weightCell, biasCell, layerOutputCells);
        end
        %plot overall network error at end of each iteration
        error = zeros(trainsetCount, outArgc);
        for t = 1:trainsetCount
            [predict, layeroutput] = ForwardNetwork(trainInp(t, :), layerOfNeurons, weightCell, biasCell);
            p(t) = predict;
            error(t, : ) = predict - trainOut(t, :);
        end
        err(iter) = (sum(error.^2)/trainsetCount)^0.5;
        figure(1);
        plot(err);
        %---Stop if reach error threshold
        if err(iter) < errorThreshhold
            break;
        end
    end
    %--Test the trained network with a test set
    testsetCount = size(testInp, 1);
    error = zeros(testsetCount, outArgc);
    for t = 1:testsetCount
        [predict, layeroutput] = ForwardNetwork(testInp(t, :), layerOfNeurons, weightCell, biasCell);
        p(t) = predict;
        error(t, : ) = predict - testRealOut(t, :);
    end
    %---Print predictions
    fprintf('Ended with %d iterations.\n', iter);
    a = testInp;
    b = testRealOut;
    c = p';
    x1_x2_act_pred_err = [a b c c-b]
    %---Plot Surface of network predictions
    testInpx1 = [-1:0.1:1];
    testInpx2 = [-1:0.1:1];
    [X1, X2] = meshgrid(testInpx1, testInpx2);
    testOutRows = size(X1, 1);
    testOutCols = size(X1, 2);
    testOut = zeros(testOutRows, testOutCols);
    for row = [1:testOutRows]
        for col = [1:testOutCols]
            test = [X1(row, col), X2(row, col)];
            [out, l] = ForwardNetwork(test, layerOfNeurons, weightCell, biasCell);
            testOut(row, col) = out;
        end
    end
    figure(2);
    surf(X1, X2, testOut);
end
%%BackPropagate: Backpropagate the output through the network and adjust weights and biases
function [weightCell, biasCell] = BackPropagate(rate, in, realOutput, sampleTarget, layer, weightCell, biasCell, layerOutputCells)
    layerCount = size(layer, 2);
    delta = cell(1, layerCount);
    D_weight = cell(1, layerCount);
    D_bias = cell(1, layerCount);
    %---From Output layer, it has different formula
    output = layerOutputCells{layerCount};
    delta{layerCount} = output .* (1-output) .* (sampleTarget - output);
    preoutput = layerOutputCells{layerCount-1};
    D_weight{layerCount} = rate .* preoutput' * delta{layerCount};
    D_bias{layerCount} = rate .* delta{layerCount};
    %---Back propagate for Hidden layers
    for layerIndex = layerCount-1:-1:1
        output = layerOutputCells{layerIndex};
        if layerIndex == 1
            preoutput = in;
        else
            preoutput = layerOutputCells{layerIndex-1};
        end
        weight = weightCell{layerIndex+1};
        sumup = (weight * delta{layerIndex+1}')';
        delta{layerIndex} = output .* (1 - output) .* sumup;
        D_weight{layerIndex} = rate .* preoutput' * delta{layerIndex};
        D_bias{layerIndex} = rate .* delta{layerIndex};
    end
    %---Update weightCell and biasCell
    for layerIndex = 1:layerCount
        weightCell{layerIndex} = weightCell{layerIndex} + D_weight{layerIndex};
        biasCell{layerIndex} = biasCell{layerIndex} + D_bias{layerIndex};
    end
end
%%ForwardNetwork: Compute feed forward neural network, Return the output and output of each neuron in each layer
function [realOutput, layerOutputCells] = ForwardNetwork(in, layer, weightCell, biasCell)
    layerCount = size(layer, 2);
    layerOutputCells = cell(1, layerCount);
    out = in;
    for layerIndex = 1:layerCount
        X = out;
        bias = biasCell{layerIndex};
        out = Sigmoid(X * weightCell{layerIndex} + bias);
        layerOutputCells{layerIndex} = out;
    end
    realOutput = out;    
end

/////////////////////////////////////////////////////////////////////////EXERCISE 6.1

p = -2 + (2-(-2)).*rand(100,1); #range of p

min = -0.5; #used for rand function
max = 0.5; #used for rand function
W1 = min + (max-min).*rand(100,1); #initial weights sigmoid layer
B1 = min + (max-min).*rand(100,1); #initial biases sigmoid layer

W2 = min + (max-min).*rand(100,1); #initial weights linear layer
B2 = min + (max-min).*rand(100,1); #initial biases linear layer

n1 = (W1.*p + B1);

S1 = 3;
a = 0.01;
#c = 0; #used for sigmoid function
#b = 1; #used for sigmoid function

gp=1+sin(p*(pi/2));

while(true) {

  logsig(n1) = 1 / (1 + exp(-n1));
  
  n2 = (W2.*logsig + B2);
  
  linear = n2
  
  convergence = linear - g
  
  if(abs(convergence) < 10^(-6)) {
    break;
  }
  
  W1 = W1 - a*S1*logsig;
  B1 = B1 - a*S1;
}



/////////////////////////////////////////////////////////EXERCISE 6.2
p = -2 + (2-(-2)).*rand(100,1); #range of p

min = -0.5; #used for rand function
max = 0.5; #used for rand function
W1 = min + (max-min).*rand(100,1); #initial weights sigmoid layer
B1 = min + (max-min).*rand(100,1); #initial biases sigmoid layer

W2 = min + (max-min).*rand(100,1); #initial weights linear layer
B2 = min + (max-min).*rand(100,1); #initial biases linear layer

n1 = (W1.*p + B1);

S1 = 15;
a = 0.01;
#c = 0; #used for sigmoid function
#b = 1; #used for sigmoid function

g(p)=1+sin(p*(pi/2));

while(true) {

  logsig(n1) = 1 / (1 + exp(-n1));
  
  n2 = (W2.*logsig + B2);
  
  linear = n2
  
  convergence = linear - g
  
  if(abs(convergence) < 10^(-6)) {
    break;
  }
  
  W1 = W1 - a*S1*logsig;
  B1 = B1 - a*S1;
}


/////////////////////////////////////////////////////////EXERCISE 8
aold=0.5; #arxikes synthikes
anew = 0;

zold=0.04; 
znew = 0;

xold=[-2 -1.5]';
xnew = 0;

g=0.1; #times parametrwn VLBP
h=1.5;
r=0.5;

#A = [3 1
#     1 3]; #gia xrhsh sthn F(x)
#B = [1 2];

Fold = 1 / (2 * ((xold'.*[3 1;1 3]).*xold)) + [1 2].*xold + 2;   
Fnew = 0;

a0 = x0;
i=0;

#Newtons Method for calculation
while(i<2) {
  
  #ypologizw gradF -> S -> L -> X ->repeat
  gradF = gradient(Fold,xold);
  sold = (-(gradF^2)^(-1)).*gradF;
  
  sold = -gradF/norm(gradF);
  L = 1; #logw tetragonikhs F

  xnew = xold + L*sold; #H ypologizw output vash propagation
  
  #ypologismos neas F
  Fnew = 1 / (2 * ((xnew'.*A).*xnew)) + B.*xnew + 2; 
  
  #allages
  while(true) {
      if(Fnew > (Fold + Fold*zold)) {
        anew = aold * r;
        g = 0;
        break;
      }
      if(Fnew < Fold) {
        anew = aold*h
        if(znew==0) {
          znew = zold;
          break;
        }
      if((Fnew > Fold) && (Fnew < Fold + Fold*z)) {
        anew = aold;
        if(znew==0) {
        znew = zold;
        }
      }
    }
  }
}

